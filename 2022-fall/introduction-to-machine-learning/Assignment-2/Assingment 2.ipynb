{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Instructions\n",
    "- Your submission should be the `.ipynb` file with your name,\n",
    "  like `YusufMesbah.ipynb`. it should include the answers to the questions in\n",
    "  markdown cells.\n",
    "- You are expected to follow the best practices for code writing and model\n",
    "training. Poor coding style will be penalized.\n",
    "- You are allowed to discuss ideas with your peers, but no sharing of code.\n",
    "Plagiarism in the code will result in failing. If you use code from the\n",
    "internet, cite it.\n",
    "- If the instructions seem vague, use common sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 1: ANN (30%)\n",
    "For this task, you are required to build a fully connect feed-forward ANN model\n",
    "for a multi-label regression problem.\n",
    "\n",
    "For the given data, you need do proper data preprocessing, design the ANN model,\n",
    "then fine-tune your model architecture (number of layers, number of neurons,\n",
    "activation function, learning rate, momentum, regularization).\n",
    "\n",
    "For evaluating your model, do $80/20$ train test split.\n",
    "\n",
    "### Data\n",
    "You will be working with the data in `Task 1.csv` for predicting students'\n",
    "scores in 3 different exams: math, reading and writing. The columns include:\n",
    " - gender\n",
    " - race\n",
    " - parental level of education\n",
    " - lunch meal plan at school\n",
    " - whether the student undertook the test preparation course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations\n",
    "\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install ray[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#Numpy: Used for getting random values for hyper-parameter tuning\n",
    "import numpy as np\n",
    "\n",
    "#Pandas: Used for loading the dataset\n",
    "import pandas as pd\n",
    "\n",
    "#Sklearn: Used for data preparation \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "#Pytorch: Used for ANN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#Ray: Used for hyper-parameter tuning\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataset\n",
    "\n",
    "df = pd.read_csv(\"Task 1.csv\")\n",
    "print(f'number of entries: {len(df)}')\n",
    "print(f'columns: {[column for column in df.columns]}')\n",
    "features = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "targets = ['math score', 'reading score', 'writing score']\n",
    "print(f\"features: {features}\")\n",
    "print(f\"targets: {targets}\")\n",
    "for feature in features:\n",
    "  values = df[feature].unique()\n",
    "  print(f'{feature}: {values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation.\n",
    "#References: Lab 2, Assignment 1 submission\n",
    "\n",
    "#Features that have no natural ordered relationship\n",
    "onehot_features = {\n",
    "    'gender': ['male', 'female'],\n",
    "    'race/ethnicity': ['group A', 'group B', 'group C', 'group D', 'group E'] \n",
    "}\n",
    "#Features that have a natural ordered relationship\n",
    "ordinal_features = {\n",
    "    'parental level of education': ['some high school', 'high school', 'some college', \"associate's degree\", \"bachelor's degree\", \"master's degree\"],\n",
    "    'test preparation course': ['none', 'completed'],\n",
    "    'lunch': ['standard', 'free/reduced']\n",
    "}\n",
    "#note that test preparation course and lunch features will be represented in the same way whether they get encoded ordinally or with one hot method\n",
    "\n",
    "#Encoding categorical data\n",
    "def ordinal_encoder(df, feature_name, categories):\n",
    "  old_column = df[feature_name]\n",
    "  old_column = np.array(old_column).reshape(-1,1)\n",
    "  encoder = OrdinalEncoder(categories=[categories])\n",
    "  new_column = encoder.fit_transform(old_column)\n",
    "  new_column = pd.DataFrame(data=new_column, columns=[feature_name])\n",
    "  new_df = df.drop(feature_name, axis=1)\n",
    "  new_df = pd.concat([new_df, new_column], axis=1)\n",
    "  return new_df\n",
    "\n",
    "def onehot_encoder(df, features_names):\n",
    "  encoder = OneHotEncoder(sparse=False, drop='first') #encoder model imported from sklearn\n",
    "  new_columns = encoder.fit_transform(df[features_names])\n",
    "  new_columns = pd.DataFrame(new_columns, dtype=int, columns=encoder.get_feature_names_out(features_names))\n",
    "  new_df = df.drop(features_names, axis=1)\n",
    "  new_df = pd.concat([new_df, new_columns], axis=1)   \n",
    "\n",
    "  return new_df\n",
    "\n",
    "def categorical_encoder(df, ordinal_features, onehot_features):\n",
    "  new_df = df\n",
    "  for key, val in ordinal_features.items():\n",
    "    new_df = ordinal_encoder(new_df, key, val)\n",
    "  onehot_features_names = []\n",
    "  for key, val in onehot_features.items():\n",
    "    onehot_features_names.append(key)\n",
    "  new_df = onehot_encoder(new_df, onehot_features_names)\n",
    "  return new_df\n",
    "\n",
    "df = categorical_encoder(df=df, ordinal_features=ordinal_features, onehot_features=onehot_features)\n",
    "\n",
    "print(f'columns: {[column for column in df.columns]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "\n",
    "X = df.iloc[:, 3:].values #Features\n",
    "y = df.iloc[:, 0:3].values #Results\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2) #split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Custom Dataset in pytorch\n",
    "#Reference: Self Practice 7\n",
    "\n",
    "class CustumData(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    super().__init__()\n",
    "    self.y = torch.tensor(y).float()\n",
    "    self.X = torch.tensor(X).float()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx, :], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN Model\n",
    "#Reference: Lab 7, https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html, https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, units = 64, layers = 10, activation = nn.ReLU()):\n",
    "    super(Net, self).__init__()\n",
    "    self.input = nn.Sequential(\n",
    "      nn.Linear(8 ,units),\n",
    "      activation\n",
    "    )\n",
    "    linear = nn.Sequential(\n",
    "      nn.Linear(units, units),\n",
    "      activation\n",
    "    )\n",
    "    self.linears = nn.ModuleList([linear for i in range(layers)])\n",
    "    self.output = nn.Linear(units, 3)\n",
    "    \n",
    "  def forward(self, x):  \n",
    "    x = self.input(x)\n",
    "    for layer in self.linears:\n",
    "      x = layer(x)\n",
    "    x = self.output(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE loss \n",
    "#Reference: https://stackoverflow.com/questions/61990363/rmse-loss-for-multi-output-regression-problem-in-pytorch, https://gist.github.com/jamesr2323/33c67ba5ac29880171b63d2c7f1acdc5\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        eps = 1e-9\n",
    "        loss = torch.sqrt(criterion(x, y) + eps)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Hyper-parameter tuning\n",
    "#Reference: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "\n",
    "def train(config):\n",
    "  activation_dict = {\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"leaky\": nn.LeakyReLU(),\n",
    "    \"elu\": nn.ELU(),\n",
    "    \"hardswish\": nn.Hardswish(),\n",
    "    \"selu\": nn.SELU(),\n",
    "    \"silu\": nn.SiLU()\n",
    "  }\n",
    "  \n",
    "  net = Net(config[\"units\"], config[\"layers\"], activation_dict[config[\"activation\"]])\n",
    "    \n",
    "  device = \"cpu\"\n",
    "  if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "  \n",
    "  net.to(device)\n",
    "\n",
    "  crit_dict = {\n",
    "    \"l1\": nn.L1Loss(),\n",
    "    \"rmse\": RMSELoss()\n",
    "  }\n",
    "\n",
    "  criterion = crit_dict[config['loss']]\n",
    "  \n",
    "  #  \"optim\":  tune.choice([\"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"sparseadam\", \"adamw\", \"asgd\", \"lbfgs\", \"nadam\", \"radam\", \"rmsprop\", \"rprop\"]),\n",
    "  optim_dict = {\n",
    "    \"sgd\": optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"]),\n",
    "    \"adam\": optim.Adam(net.parameters(), lr=config['lr']),\n",
    "    \"adadelta\": optim.Adadelta(net.parameters(), lr=config['lr']),\n",
    "    \"adagrad\": optim.Adagrad(net.parameters(), lr=config['lr']),\n",
    "    \"adamw\": optim.AdamW(net.parameters(), lr=config['lr']),\n",
    "    \"asgd\": optim.ASGD(net.parameters(), lr=config['lr']),\n",
    "    \"nadam\": optim.NAdam(net.parameters(), lr=config['lr']),\n",
    "    \"radam\": optim.RAdam(net.parameters(), lr=config['lr']),\n",
    "    \"rmsprop\": optim.RMSprop(net.parameters(), lr=config['lr'], momentum=config[\"momentum\"]),\n",
    "    \"rprop\": optim.Rprop(net.parameters(), lr=config['lr'])\n",
    "  }\n",
    "  optimizer = optim_dict[config[\"optim\"]]\n",
    "\n",
    "  trainset = CustumData(X_train, y_train)\n",
    "  testset = CustumData(X_test, y_test) \n",
    "\n",
    "  trainloader = DataLoader(trainset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
    "  testloader = DataLoader(testset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n",
    "  \n",
    "  #Reduce learning rate when a metric has stopped improving\n",
    "  schedular = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, cooldown=5)\n",
    "    \n",
    "  min_loss = 1e18\n",
    "  \n",
    "  for epoch in range(200):  # loop over the dataset multiple times\n",
    "    #Training\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "      inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "    \n",
    "      outputs = net(inputs)\n",
    "            \n",
    "      loss = criterion(outputs, labels)\n",
    "            \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item() * len(inputs)\n",
    "        \n",
    "    schedular.step(train_loss / len(trainloader.dataset))\n",
    "    \n",
    "    #Validation/Testing\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in testloader:\n",
    "      with torch.no_grad():\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "                \n",
    "        test_loss += loss.item() * len(inputs)\n",
    "        \n",
    "    epoch_loss = test_loss/len(testloader.dataset)\n",
    "    if epoch_loss < min_loss:\n",
    "      min_loss = epoch_loss\n",
    "\n",
    "  tune.report(loss=min_loss)\n",
    "  print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "\n",
    "def main_task1(num_samples=20, max_num_epochs=10):\n",
    "  config = {\n",
    "    \"units\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "    \"layers\": tune.sample_from(lambda _: np.random.randint(1, 30)),\n",
    "    \"activation\": tune.choice([\"relu\", \"leaky\", \"elu\", \"hardswish\", \"selu\", \"silu\"]),\n",
    "    \"optim\":  tune.choice([\"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"adamw\", \"asgd\",  \"nadam\", \"radam\", \"rmsprop\", \"rprop\"]),\n",
    "    \"loss\": tune.choice([\"l1\", \"rmse\"]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"momentum\": tune.sample_from(lambda _: np.random.uniform(0,1.0)),\n",
    "    \"batch_size\": tune.sample_from(lambda _: 2 ** np.random.randint(5, 10)),\n",
    "  }\n",
    "  reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\"])\n",
    "  result = tune.run(\n",
    "    train,\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    progress_reporter=reporter\n",
    "  )\n",
    "  best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "  print(\"Best trial config: {}\".format(best_trial.config))\n",
    "  print(\"Best trial final validation loss: {}\".format(\n",
    "    best_trial.last_result[\"loss\"]))\n",
    "    \n",
    "main_task1(num_samples=200, max_num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questions\n",
    "1. What preprocessing techniques did you use? Why?\n",
    "    - Encoding:\n",
    "      - OneHotEncoding\n",
    "        - Features: \n",
    "          - gender \n",
    "          - race/ethnicity \n",
    "        - Reasoning: \n",
    "          - Because gender and race do not have a natural ordered relationship in themselves. So, one hot encoding is the appropriate here.\n",
    "      - OrdinalEncoding:\n",
    "        - Features: \n",
    "          - parental level of education  \n",
    "          - test preparation course \n",
    "          - lunch \n",
    "        - Reasoning: \n",
    "          - parental level of education: there's a natural order from uneducated to educated.\n",
    "          - Order:\n",
    "            - some high school -> 0  \n",
    "            - high school -> 1 \n",
    "            - some college -> 2 \n",
    "            - associate's degree -> 3 \n",
    "            - bachelor's degree -> 4 \n",
    "            - master's degree -> 5 \n",
    "          - lunch: there's a natural order in terms of what is better, free/reduced or full priced. \n",
    "          - Order:\n",
    "            - standard -> 0 \n",
    "            - free/reduced -> 1 \n",
    "          - test preparation course: there's a natural order in terms of who is prepared and not.\n",
    "          - Order:\n",
    "            - none -> 0 \n",
    "            - completed -> 1 \n",
    "      - Note: For the lunch and test preparation course features, both OneHotEncoding and OrdinalEncoding are valid due to the fact that there's only 2 possible values for each feature.\n",
    "    - Scaling\n",
    "      - Used standard scaler to scale the dataset\n",
    "      - Reasoning: to make all features contribute equally instead of having ordinally encoded features(parental level of education) have more weight than other features.\n",
    "2. Describe the fine-tuning process and how you reached your model architecture.\n",
    "    - Fine-tuning: \n",
    "      - Used ray for hyperparamter tuning\n",
    "        - Reference: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "      - Decided to tune the following:\n",
    "        - Number of hidden layers: which is\n",
    "          - Possible values: \n",
    "            - 2^i where i is a random integer between 2 and 9 inclusive\n",
    "        - Number of neurons in hidden layers: which is\n",
    "          - Possible values: \n",
    "            - random integer between 1 and 30 inclusive\n",
    "        - Learning rate: which is\n",
    "          - Possible values: \n",
    "            - random float between 0.1 and 0.00001 inclusive\n",
    "        - Momuntem: which is\n",
    "          - Possible values: \n",
    "            - random float between 0.0 and 1.0 inclusive\n",
    "        - Activiation Functions: which is\n",
    "          - Possible values: \n",
    "            - ReLU\n",
    "            - LeakyReLU\n",
    "            - Hardswish\n",
    "            - ELU\n",
    "            - SELU\n",
    "            - SiLU\n",
    "        - Optimizers: which is \n",
    "          - Possible values:\n",
    "            - SGD\n",
    "            - Adam\n",
    "            - Adadelta\n",
    "            - Adagrad\n",
    "            - AdamW\n",
    "            - ASGD\n",
    "            - NAdam\n",
    "            - RAdam\n",
    "            - RMSprop\n",
    "            - Rprop\n",
    "        - Loss Functions: which is\n",
    "          - Possible values:\n",
    "            - L1Loss\n",
    "            - RMSELoss\n",
    "        - Batch size: which is:\n",
    "          - Possible values:\n",
    "            - 2^i where i is a random integer between 5 and 10 inclusive\n",
    "      - Also used ReduceLROnPlateau to modify the lr in between epochs\n",
    "      - Reference: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "    - After running for 200 samples where each sample ran for 200 epochs, the best parameters reached by the hyper paramter tuning is:\n",
    "      - Number of hidden layers: 1 \n",
    "      - Number of neurons: 32\n",
    "      - Learning rate: 0.025644775234438002\n",
    "      - Momuntem: 0.2103397790313165\n",
    "      - Activiation Function: SiLU\n",
    "      - Optimizer: ASGD\n",
    "      - Loss Function: L1Loss\n",
    "      - Batch size: 64\n",
    "    - Which produced validation loss equal to 10.603941497802735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 2: CNN (40%)\n",
    "For this task, you will be doing image classification:\n",
    "- First, adapt your best model from Task 1 to work on this task, and\n",
    "fit it on the new data. Then, evaluate its performance.\n",
    "- After that, build a CNN model for image classification.\n",
    "- Compare both models in terms of accuracy, number of parameters and speed of\n",
    "inference (the time the model takes to predict 50 samples).\n",
    "\n",
    "For the given data, you need to do proper data preprocessing and augmentation,\n",
    "data loaders.\n",
    "Then fine-tune your model architecture (number of layers, number of filters,\n",
    "activation function, learning rate, momentum, regularization).\n",
    "\n",
    "### Data\n",
    "You will be working with the data in `triple_mnist.zip` for predicting 3-digit\n",
    "numbers writen in the image. Each image contains 3 digits similar to the\n",
    "following example (whose label is `039`):\n",
    "\n",
    "![example](https://github.com/shaohua0116/MultiDigitMNIST/blob/master/asset/examples/039/0_039.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#Numpy: Used for getting random values for hyper-parameter tuning\n",
    "import numpy as np\n",
    "\n",
    "#Pandas: Used for loading the dataset\n",
    "import pandas as pd\n",
    "\n",
    "#Sklearn: Used for data preparation \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "#Pytorch: Used for ANN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#Ray: Used for hyper-parameter tuning\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/task2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/iviosab/Desktop/GitHub/f22-iml/Assignment-2/Assingment 2.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/iviosab/Desktop/GitHub/f22-iml/Assignment-2/Assingment%202.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Get the dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/iviosab/Desktop/GitHub/f22-iml/Assignment-2/Assingment%202.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#Reference: https://stackoverflow.com/questions/3451111/unzipping-files-in-python\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/iviosab/Desktop/GitHub/f22-iml/Assignment-2/Assingment%202.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(\u001b[39m'\u001b[39m\u001b[39mtriple_mnist.zip\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m zip_ref:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/iviosab/Desktop/GitHub/f22-iml/Assignment-2/Assingment%202.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     zip_ref\u001b[39m.\u001b[39;49mextractall(\u001b[39m'\u001b[39;49m\u001b[39m/task2/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1645\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[1;32m   1644\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[0;32m-> 1645\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1691\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1689\u001b[0m upperdirs \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(targetpath)\n\u001b[1;32m   1690\u001b[0m \u001b[39mif\u001b[39;00m upperdirs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(upperdirs):\n\u001b[0;32m-> 1691\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(upperdirs)\n\u001b[1;32m   1693\u001b[0m \u001b[39mif\u001b[39;00m member\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m   1694\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(targetpath):\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/task2'"
     ]
    }
   ],
   "source": [
    "#Get the dataset\n",
    "#Reference: https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
    "with zipfile.ZipFile('triple_mnist.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/task2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Custom Dataset in pytorch\n",
    "#Reference: Self Practice 7\n",
    "\n",
    "class CustumData(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    super().__init__()\n",
    "    self.y = torch.tensor(y).float()\n",
    "    self.X = torch.tensor(X).float()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx, :], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Questions\n",
    "1. What preprocessing techniques did you use? Why?\n",
    "    - *Answer*\n",
    "2. What data augmentation techniques did you use?\n",
    "    - *Answer*\n",
    "3. Describe the fine-tuning process and how you reached your final CNN model.\n",
    "    - *Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 3: Decision Trees and Ensemble Learning (15%)\n",
    "\n",
    "For the `loan_data.csv` data, predict if the bank should give a loan or not.\n",
    "You need to do the following:\n",
    "- Fine-tune a decision tree on the data\n",
    "- Fine-tune a random forest on the data\n",
    "- Compare their performance\n",
    "- Visualize your DT and one of the trees from the RF\n",
    "\n",
    "For evaluating your models, do $80/20$ train test split.\n",
    "\n",
    "### Data\n",
    "- `credit.policy`: Whether the customer meets the credit underwriting criteria.\n",
    "- `purpose`: The purpose of the loan.\n",
    "- `int.rate`: The interest rate of the loan.\n",
    "- `installment`: The monthly installments owed by the borrower if the loan is funded.\n",
    "- `log.annual.inc`: The natural logarithm of the self-reported annual income of the borrower.\n",
    "- `dti`: The debt-to-income ratio of the borrower.\n",
    "- `fico`: The FICO credit score of the borrower.\n",
    "- `days.with.cr.line`: The number of days the borrower has had a credit line.\n",
    "- `revol.bal`: The borrower's revolving balance.\n",
    "- `revol.util`: The borrower's revolving line utilization rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questions\n",
    "1. How did the DT compare to the RF in performance? Why?\n",
    "    - *Answer*\n",
    "2. After fine-tuning, how does the max depth in DT compare to RF? Why?\n",
    "    - *Answer*\n",
    "3. What is ensemble learning? What are its pros and cons?\n",
    "    - *Answer*\n",
    "4. Briefly explain 2 types of boosting methods and 2 types of bagging methods.\n",
    "Which of these categories does RF fall under?\n",
    "    - *Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 4: Domain Gap (15%)\n",
    "\n",
    "Evaluate your CNN model from task 2 on SVHN data without retraining your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Questions\n",
    "1. How did your model perform? Why is it better/worse?\n",
    "    - *Answer*\n",
    "2. What is domain gap in the context of ML?\n",
    "    - *Answer*\n",
    "3. Suggest two ways through which the problem of domain gap can be tackled.\n",
    "    - *Answer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
